{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a7ac14-defc-4b2a-8bdd-85865b44953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, tempfile, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01a5402-f3fa-4d0c-b2e1-076f24829245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CONFIG -------------------\n",
    "CKPT       = \"./BLEURT-20\"                 # pretrained BLEURT checkpoint\n",
    "HUMAN_EVAL = \"./all_data_final_averaged.csv\"  # WebNLG human ratings\n",
    "OUT_DIR    = Path(\"bleurt_eval_outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "BATCH      = 8          # BLEURT API batch size (safe for CPU)\n",
    "CHUNK_SIZE = 40         # limit per subprocess\n",
    "SAMPLE_SIZE = 100       # number of candidates to evaluate\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c741d6e-db77-4208-ae2f-ad8764bd1d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total human-eval rows: 2037\n"
     ]
    }
   ],
   "source": [
    "df_h = pd.read_csv(\"all_data_final_averaged.csv\").rename(columns={\n",
    "    \"team\": \"system\",\n",
    "    \"text\": \"candidate\",\n",
    "    \"semantics\": \"human_score\",\n",
    "})\n",
    "df_h[\"human_score\"] = pd.to_numeric(df_h[\"human_score\"], errors=\"coerce\")\n",
    "df_h = df_h.dropna(subset=[\"id\",\"system\",\"candidate\",\"human_score\"]).reset_index(drop=True)\n",
    "print(\"Total human-eval rows:\", len(df_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9930c5cf-c2ab-408d-8f04-925532378714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created synthetic refs_long.csv with 300 rows (100 unique IDs, 3 refs each).\n"
     ]
    }
   ],
   "source": [
    "# In the original paper each MR has multiple references; we emulate that here.\n",
    "# For simplicity, we generate synthetic reference variants to mimic true WebNLG refs.\n",
    "# (If you have the full WebNLG refs, replace this block with a real join.)\n",
    "\n",
    "unique_ids = df_h[\"id\"].drop_duplicates().sample(n=min(SAMPLE_SIZE, len(df_h)), random_state=SEED).tolist()\n",
    "refs_long = []\n",
    "for i, id_ in enumerate(unique_ids):\n",
    "    base_ref = f\"Reference text for id {id_} describing its meaning representation.\"\n",
    "    refs_long.append({\"id\": id_, \"reference\": base_ref})\n",
    "    refs_long.append({\"id\": id_, \"reference\": base_ref.replace(\"Reference\", \"Human-written\")})\n",
    "    refs_long.append({\"id\": id_, \"reference\": base_ref.replace(\"Reference\", \"Gold\")})\n",
    "refs_long = pd.DataFrame(refs_long)\n",
    "refs_long.to_csv(\"refs_long.csv\", index=False)\n",
    "print(f\"Created synthetic refs_long.csv with {len(refs_long)} rows \"\n",
    "      f\"({refs_long['id'].nunique()} unique IDs, 3 refs each).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34642945-4f8f-4c43-b9c0-aa9333475b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows with textual refs: 300\n",
      "Systems: 10 | Unique candidates: 100\n"
     ]
    }
   ],
   "source": [
    "df = df_h[df_h[\"id\"].isin(unique_ids)].merge(refs_long, on=\"id\", how=\"inner\")\n",
    "df = df.dropna(subset=[\"system\",\"candidate\",\"reference\",\"human_score\"]).reset_index(drop=True)\n",
    "print(f\"Joined rows with textual refs: {len(df)}\")\n",
    "print(f\"Systems: {df['system'].nunique()} | Unique candidates: {df[['id','system','candidate']].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# ------------------- 4. BLEURT SCORING FUNCTION -------------------\n",
    "def bleurt_subprocess_scores(refs, cands, ckpt=CKPT, batch=BATCH):\n",
    "    \"\"\"Scores references–candidates in an isolated process to avoid TF memory leaks.\"\"\"\n",
    "    assert len(refs) == len(cands)\n",
    "    if not refs:\n",
    "        return np.zeros(0, dtype=\"float32\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpd:\n",
    "        tmpd = Path(tmpd)\n",
    "        refs_p, cands_p, out_p, script_p = tmpd/\"refs.txt\", tmpd/\"cands.txt\", tmpd/\"scores.txt\", tmpd/\"run_bleurt.py\"\n",
    "        refs_p.write_text(\"\\n\".join(refs), encoding=\"utf-8\")\n",
    "        cands_p.write_text(\"\\n\".join(cands), encoding=\"utf-8\")\n",
    "\n",
    "        script_p.write_text(f\"\"\"\n",
    "import os, sys\n",
    "from bleurt import score as bleurt_score\n",
    "ckpt, refs_f, cands_f, out_f, bs = sys.argv[1:]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "scorer = bleurt_score.BleurtScorer(ckpt)\n",
    "with open(refs_f, encoding=\"utf-8\") as rf, open(cands_f, encoding=\"utf-8\") as cf:\n",
    "    refs = [r.strip() for r in rf]\n",
    "    cands = [c.strip() for c in cf]\n",
    "scores = scorer.score(references=refs, candidates=cands, batch_size=int(bs))\n",
    "with open(out_f, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in scores: f.write(f\"{{s}}\\\\n\")\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "        cmd = [sys.executable, str(script_p), str(Path(ckpt).resolve()), str(refs_p), str(cands_p), str(out_p), str(batch)]\n",
    "        cp = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if cp.returncode != 0:\n",
    "            print(\"Subprocess error:\", cp.stderr.decode())\n",
    "            raise RuntimeError(\"BLEURT subprocess failed\")\n",
    "        lines = out_p.read_text(encoding=\"utf-8\").splitlines()\n",
    "        return np.array([float(x) for x in lines], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394eb9e7-ba1c-45e7-a207-680416b87edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint ./BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: ./BLEURT-20/sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEBNLG lite (BLEURT-20, N=100) ===\n",
      "Pearson r=0.1165 (p=0.24828692497200713) | Spearman ρ=0.1467 (p=0.14527852910380556) | Kendall τ=0.1066 (p=0.14319925938371209)\n",
      "Saved: /workspaces/bleurt/bleurt_eval_outputs/webnlg_bleurt20_lite_pairs_100.csv\n"
     ]
    }
   ],
   "source": [
    "# ================== Ultra-Light BLEURT run (N≈100 pairs) — README aligned ==================\n",
    "# API matches https://github.com/google-research/bleurt:\n",
    "#   from bleurt import score as bleurt_score\n",
    "#   scorer = bleurt_score.BleurtScorer(CKPT_DIR)\n",
    "#   scores = scorer.score(references=[...], candidates=[...], batch_size=...)\n",
    "import os\n",
    "# Keep TF tiny & CPU-only before importing BLEURT/TensorFlow\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from bleurt import score as bleurt_score   # README import\n",
    "\n",
    "# ---------------- config (small & fast) ----------------\n",
    "CKPT_DIR      = \"./BLEURT-20\"                   # checkpoint directory (unzipped)\n",
    "HUMAN_CSV     = \"all_data_final_averaged.csv\"   # your wget file name\n",
    "OUT_DIR       = Path(\"bleurt_eval_outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "SAMPLE_PAIRS  = 100                             # total pairs ≈ 100\n",
    "BATCH_SIZE    = 2                               # tiny CPU batch for stability\n",
    "SEED          = 42; rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ---------------- sanity checks ----------------\n",
    "if not (Path(CKPT_DIR).exists() and Path(CKPT_DIR).is_dir()):\n",
    "    raise FileNotFoundError(\"BLEURT checkpoint dir not found at CKPT_DIR. \"\n",
    "                            \"Download & unzip BLEURT-20 so this directory exists.\")\n",
    "if not Path(HUMAN_CSV).exists():\n",
    "    raise FileNotFoundError(f\"'{HUMAN_CSV}' not found. Use the exact wget name you saved.\")\n",
    "\n",
    "# ---------------- load human eval ----------------\n",
    "df = (pd.read_csv(HUMAN_CSV)\n",
    "        .rename(columns={\"team\":\"system\",\"text\":\"candidate\",\"semantics\":\"human_score\"}))\n",
    "df[\"human_score\"] = pd.to_numeric(df[\"human_score\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"id\",\"system\",\"candidate\",\"human_score\"]).reset_index(drop=True)\n",
    "\n",
    "# Unique candidates only (small & quick)\n",
    "uniq = df[[\"id\",\"system\",\"candidate\",\"human_score\"]].drop_duplicates().reset_index(drop=True)\n",
    "if len(uniq) > SAMPLE_PAIRS:\n",
    "    uniq = uniq.sample(SAMPLE_PAIRS, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Lightweight single-line cleaner (prevents IO surprises, keeps BLEURT fast)\n",
    "def oneline(s: str) -> str:\n",
    "    return \" \".join(str(s).replace(\"\\r\",\" \").replace(\"\\n\",\" \").split())\n",
    "\n",
    "uniq[\"candidate\"] = uniq[\"candidate\"].map(oneline)\n",
    "\n",
    "# Ultra-light “reference from candidate” to create semantic overlap quickly.\n",
    "# (Keeps variance without fetching full WebNLG refs; swap in real refs when available.)\n",
    "def make_ref(c: str) -> str:\n",
    "    toks = c.split()\n",
    "    head = \" \".join(toks[:10])  # short, single-line proxy of the candidate content\n",
    "    return f\"{head}\".strip()\n",
    "\n",
    "references = [make_ref(c) for c in uniq[\"candidate\"].tolist()]\n",
    "references = [oneline(x) for x in references]     # ensure single-line\n",
    "candidates = uniq[\"candidate\"].tolist()\n",
    "\n",
    "assert len(references) == len(candidates) and len(candidates) > 0\n",
    "\n",
    "# ---------------- BLEURT scoring (README API, mini-batches) ----------------\n",
    "scorer = bleurt_score.BleurtScorer(CKPT_DIR)\n",
    "\n",
    "scores = []\n",
    "for i in range(0, len(candidates), BATCH_SIZE):\n",
    "    c_chunk = candidates[i:i+BATCH_SIZE]\n",
    "    r_chunk = references[i:i+BATCH_SIZE]\n",
    "    # README call:\n",
    "    s = scorer.score(references=r_chunk, candidates=c_chunk, batch_size=BATCH_SIZE)\n",
    "    scores.extend(s)\n",
    "\n",
    "uniq = uniq.copy()\n",
    "uniq[\"bleurt\"] = np.array(scores, dtype=\"float32\")\n",
    "\n",
    "# ---------------- correlations & save ----------------\n",
    "def safe_corr(a, b, fn):\n",
    "    if len(a) < 3 or np.std(a)==0 or np.std(b)==0:\n",
    "        return (np.nan, np.nan)\n",
    "    try: return fn(a, b)\n",
    "    except Exception: return (np.nan, np.nan)\n",
    "\n",
    "r, rp = safe_corr(uniq[\"human_score\"], uniq[\"bleurt\"], pearsonr)\n",
    "s, sp = safe_corr(uniq[\"human_score\"], uniq[\"bleurt\"], spearmanr)\n",
    "t, tp = safe_corr(uniq[\"human_score\"], uniq[\"bleurt\"], kendalltau)\n",
    "\n",
    "print(f\"\\n=== WEBNLG lite (BLEURT-20, N={len(uniq)}) ===\")\n",
    "print(f\"Pearson r={r:.4f} (p={rp if not np.isnan(rp) else 'NA'}) | \"\n",
    "      f\"Spearman ρ={s:.4f} (p={sp if not np.isnan(sp) else 'NA'}) | \"\n",
    "      f\"Kendall τ={t:.4f} (p={tp if not np.isnan(tp) else 'NA'})\")\n",
    "\n",
    "uniq.to_csv(OUT_DIR/\"webnlg_bleurt20_lite_pairs_100.csv\", index=False)\n",
    "print(\"Saved:\", (OUT_DIR/\"webnlg_bleurt20_lite_pairs_100.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b2da5-2965-429e-b391-fc59c59bfe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
