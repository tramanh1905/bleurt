{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ea254d-1bd6-4e17-9c0e-467533dbecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (108, 7)\n",
      "Columns: ['id', 'topic', 'dialogue_text', 'candidate_summary', 'reference_summary_1', 'reference_summary_2', 'human_score']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>dialogue_text</th>\n",
       "      <th>candidate_summary</th>\n",
       "      <th>reference_summary_1</th>\n",
       "      <th>reference_summary_2</th>\n",
       "      <th>human_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manual_001</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>JAMES: Good morning, Professor Austin, how are...</td>\n",
       "      <td>James greets Professor Austin, presents Emma, ...</td>\n",
       "      <td>James introduces Emma to Professor Austin and ...</td>\n",
       "      <td>James greets Professor Austin, presents Emma, ...</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual_002</td>\n",
       "      <td>informal_greetings</td>\n",
       "      <td>JANE: Hi, Helen! How’s it going? HELEN: Fine, ...</td>\n",
       "      <td>Two friends exchange greetings, discuss study ...</td>\n",
       "      <td>Jane greets Helen, learns she is heading to th...</td>\n",
       "      <td>Two friends exchange greetings, discuss study ...</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manual_003</td>\n",
       "      <td>formal_introductions</td>\n",
       "      <td>MARGARET: Mr. Wilson, I’d like you to meet Dr....</td>\n",
       "      <td>A formal introduction leads to a brief exchang...</td>\n",
       "      <td>Margaret formally introduces Mr. Wilson and Dr...</td>\n",
       "      <td>A formal introduction leads to a brief exchang...</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                 topic  \\\n",
       "0  manual_001      formal_greetings   \n",
       "1  manual_002    informal_greetings   \n",
       "2  manual_003  formal_introductions   \n",
       "\n",
       "                                       dialogue_text  \\\n",
       "0  JAMES: Good morning, Professor Austin, how are...   \n",
       "1  JANE: Hi, Helen! How’s it going? HELEN: Fine, ...   \n",
       "2  MARGARET: Mr. Wilson, I’d like you to meet Dr....   \n",
       "\n",
       "                                   candidate_summary  \\\n",
       "0  James greets Professor Austin, presents Emma, ...   \n",
       "1  Two friends exchange greetings, discuss study ...   \n",
       "2  A formal introduction leads to a brief exchang...   \n",
       "\n",
       "                                 reference_summary_1  \\\n",
       "0  James introduces Emma to Professor Austin and ...   \n",
       "1  Jane greets Helen, learns she is heading to th...   \n",
       "2  Margaret formally introduces Mr. Wilson and Dr...   \n",
       "\n",
       "                                 reference_summary_2  human_score  \n",
       "0  James greets Professor Austin, presents Emma, ...         3.08  \n",
       "1  Two friends exchange greetings, discuss study ...         2.74  \n",
       "2  A formal introduction leads to a brief exchang...         3.19  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, re, json\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(\"dialoguesum_lite_100_clean_improved.csv\")\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760f9b2-098c-4bae-a67b-06df61bb9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    if pd.isna(s): \n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.encode(\"utf-8\",\"ignore\").decode(\"utf-8\",\"ignore\")\n",
    "    s = re.sub(r\"[^\\S\\r\\n]+\", \" \", s)    # collapse multiple spaces/tabs\n",
    "    s = re.sub(r\"\\s*\\n\\s*\", \" \", s)      # merge newlines\n",
    "    s = s.replace(\"…\",\"...\").strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5644c007-68fc-455d-871f-0271121d4d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (95, 7)\n"
     ]
    }
   ],
   "source": [
    "# Clean text fields\n",
    "df[\"dialogue_text\"] = df[\"dialogue_text\"].apply(clean_text)\n",
    "df[\"reference_summary_1\"] = df[\"reference_summary_1\"].apply(clean_text)\n",
    "df[\"reference_summary_2\"] = df[\"reference_summary_2\"].apply(clean_text)\n",
    "\n",
    "# Drop rows where dialogue or summary is too short\n",
    "df = df[(df[\"dialogue_text\"].str.len() > 20) & (df[\"reference_summary_1\"].str.len() > 10)]\n",
    "df = df.drop_duplicates(subset=[\"dialogue_text\",\"reference_summary_1\"])\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"After cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cf764a-4e8e-4bbd-8986-be73d0d6ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded shape: (190, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>candidate</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manual_001</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>JAMES: Good morning, Professor Austin, how are...</td>\n",
       "      <td>James introduces Emma to Professor Austin and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual_001</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>JAMES: Good morning, Professor Austin, how are...</td>\n",
       "      <td>James greets Professor Austin, presents Emma, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manual_002</td>\n",
       "      <td>informal_greetings</td>\n",
       "      <td>JANE: Hi, Helen! How’s it going? HELEN: Fine, ...</td>\n",
       "      <td>Jane greets Helen, learns she is heading to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id               topic  \\\n",
       "0  manual_001    formal_greetings   \n",
       "1  manual_001    formal_greetings   \n",
       "2  manual_002  informal_greetings   \n",
       "\n",
       "                                           candidate  \\\n",
       "0  JAMES: Good morning, Professor Austin, how are...   \n",
       "1  JAMES: Good morning, Professor Austin, how are...   \n",
       "2  JANE: Hi, Helen! How’s it going? HELEN: Fine, ...   \n",
       "\n",
       "                                           reference  \n",
       "0  James introduces Emma to Professor Austin and ...  \n",
       "1  James greets Professor Austin, presents Emma, ...  \n",
       "2  Jane greets Helen, learns she is heading to th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    for ref in [r.reference_summary_1, r.reference_summary_2]:\n",
    "        rows.append({\n",
    "            \"id\": r.id,\n",
    "            \"topic\": r.topic,\n",
    "            \"candidate\": r.dialogue_text,\n",
    "            \"reference\": ref,\n",
    "        })\n",
    "df_long = pd.DataFrame(rows)\n",
    "print(\"Expanded shape:\", df_long.shape)\n",
    "df_long.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816325a2-f8e1-4b53-b2d3-a9a49027330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - /workspaces/bleurt/dialoguesum_bleurt_ready/dialoguesum_bleurt_ready.csv\n",
      " - /workspaces/bleurt/dialoguesum_bleurt_ready/dialoguesum_bleurt_ready.jsonl\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"./dialoguesum_bleurt_ready\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "csv_path  = out_dir / \"dialoguesum_bleurt_ready.csv\"\n",
    "jsonl_path = out_dir / \"dialoguesum_bleurt_ready.jsonl\"\n",
    "\n",
    "df_long.to_csv(csv_path, index=False)\n",
    "df_long.to_json(jsonl_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", csv_path.resolve())\n",
    "print(\" -\", jsonl_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4474b37-6a23-44ac-bf66-a9f3408a04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 07:18:06.230075: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-01 07:18:28.760233: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-01 07:18:41.642859: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint ./BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: ./BLEURT-20/sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 07:18:49.558384: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-11-01 07:18:50.928318: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 256307200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "2025-11-01 07:21:56.979935: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 23592960 exceeds 10% of free system memory.\n",
      "2025-11-01 07:21:57.029148: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 23592960 exceeds 10% of free system memory.\n",
      "2025-11-01 07:21:57.030062: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 23592960 exceeds 10% of free system memory.\n",
      "2025-11-01 07:21:57.387203: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 23592960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>bleurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>author_072</td>\n",
       "      <td>job_interview</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>manual_added_004</td>\n",
       "      <td>apartment rental</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>author_030</td>\n",
       "      <td>landlord_tenant</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>author_006</td>\n",
       "      <td>restaurant_order</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>author_024</td>\n",
       "      <td>bank_inquiry</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>manual_001w_1</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>manual_005</td>\n",
       "      <td>time_and_plans</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>manual_001w_2</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>author_046</td>\n",
       "      <td>restaurant_order</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>author_038</td>\n",
       "      <td>movie_plans</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id             topic  bleurt\n",
       "175        author_072     job_interview   0.529\n",
       "180  manual_added_004  apartment rental   0.556\n",
       "111        author_030   landlord_tenant   0.529\n",
       "65         author_006  restaurant_order   0.492\n",
       "101        author_024      bank_inquiry   0.523\n",
       "15      manual_001w_1  formal_greetings   0.358\n",
       "9          manual_005    time_and_plans   0.509\n",
       "16      manual_001w_2  formal_greetings   0.357\n",
       "141        author_046  restaurant_order   0.608\n",
       "124        author_038       movie_plans   0.482"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bleurt import score as bleurt_score\n",
    "import numpy as np\n",
    "\n",
    "ckpt = \"./BLEURT-20\"   # or bleurt-large-512\n",
    "scorer = bleurt_score.BleurtScorer(ckpt)\n",
    "\n",
    "sample = df_long.sample(10, random_state=42)\n",
    "scores = scorer.score(candidates=sample[\"candidate\"].tolist(),\n",
    "                      references=sample[\"reference\"].tolist())\n",
    "sample[\"bleurt\"] = np.round(scores,3)\n",
    "sample[[\"id\",\"topic\",\"bleurt\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a26314-cf71-48b1-9cd4-2dbda9e44306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEURT-20.zip]\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of BLEURT-20.zip or\n",
      "        BLEURT-20.zip.zip, and cannot find BLEURT-20.zip.ZIP, period.\n",
      "bert_config.json    saved_model.pb    sent_piece.vocab\n",
      "bleurt_config.json  sent_piece.model  variables\n"
     ]
    }
   ],
   "source": [
    "# Create folder for checkpoints\n",
    "!mkdir -p BLEURT-20\n",
    "\n",
    "# Download and unzip the BLEURT-20 model (recommended medium-sized checkpoint)\n",
    "!wget -q https://storage.googleapis.com/bleurt-oss/bleurt-20.zip -O BLEURT-20.zip\n",
    "!unzip -oq BLEURT-20.zip -d BLEURT-20\n",
    "!rm BLEURT-20.zip\n",
    "\n",
    "# Confirm the files\n",
    "!ls BLEURT-20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbcdd745-9f55-417e-847c-ddbaf733c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Contents: ['saved_model.pb', 'variables', 'bert_config.json', 'sent_piece.model', 'bleurt_config.json', 'sent_piece.vocab']\n",
      "Has all required files: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ckpt = Path(\"BLEURT-20\")\n",
    "print(\"Exists:\", ckpt.exists())\n",
    "print(\"Contents:\", [p.name for p in ckpt.iterdir()])\n",
    "need = [\"saved_model.pb\", \"bleurt_config.json\", \"variables\"]\n",
    "print(\"Has all required files:\", all((ckpt / n).exists() for n in need))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4673b641-4344-4c61-8a29-983daf097b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n",
      "2025-10-30 10:29:55.272417: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 256307200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7462440729141235, 0.7786075472831726]\n"
     ]
    }
   ],
   "source": [
    "from bleurt import score as bleurt_score\n",
    "scorer = bleurt_score.BleurtScorer(\"BLEURT-20\")   # <-- use the folder you just verified\n",
    "\n",
    "print(scorer.score(\n",
    "    candidates=[\"The cat is on the mat.\", \"He plays piano.\"],\n",
    "    references=[\"A cat sits on a mat.\", \"He is playing the piano.\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12068b00-e721-4791-8e64-ca7ed8412808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will:\n",
    "#  - look for any \"*bleurt*ready*.csv\" under the workspace\n",
    "#  - otherwise rebuild the file from dialoguesum_lite_100_clean.csv (or similar)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, re\n",
    "\n",
    "TARGET_DIR = Path(\"dialoguesum_bleurt_ready\")\n",
    "TARGET_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TARGET_PATH = TARGET_DIR / \"dialoguesum_bleurt_ready.csv\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).encode(\"utf-8\",\"ignore\").decode(\"utf-8\",\"ignore\")\n",
    "    s = re.sub(r\"[^\\S\\r\\n]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s*\\n\\s*\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 1) Try to find an existing bleurt-ready file anywhere\n",
    "hits = sorted(Path(\".\").rglob(\"*bleurt*ready*.csv\"))\n",
    "if hits:\n",
    "    TARGET_PATH = hits[0]\n",
    "    print(\"✅ Found existing BLEURT-ready file:\", TARGET_PATH)\n",
    "\n",
    "else:\n",
    "    # 2) Rebuild from the clean 100-row dataset\n",
    "    base = None\n",
    "    patterns = [\n",
    "        \"dialoguesum_lite_100_clean.csv\",\n",
    "        \"dialoguesum_lite_100.csv\",\n",
    "        \"*lite*100*clean*.csv\",\n",
    "        \"*lite*100*.csv\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = sorted(Path(\".\").rglob(pat))\n",
    "        if m:\n",
    "            base = m[0]\n",
    "            break\n",
    "\n",
    "    if base is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the base dataset (e.g., dialoguesum_lite_100_clean.csv). \"\n",
    "            \"Search your workspace or adjust the patterns above.\"\n",
    "        )\n",
    "\n",
    "    print(\"Building BLEURT-ready pairs from:\", base)\n",
    "    df = pd.read_csv(base)\n",
    "\n",
    "    # Clean & filter\n",
    "    for c in [\"dialogue_text\",\"reference_summary_1\",\"reference_summary_2\"]:\n",
    "        df[c] = df[c].apply(clean_text)\n",
    "    df = df[(df[\"dialogue_text\"].str.len() > 20) & (df[\"reference_summary_1\"].str.len() > 10)].copy()\n",
    "\n",
    "    # Expand to candidate–reference rows (two refs per dialogue)\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        rows.append({\"id\": r.id, \"topic\": r.topic, \"candidate\": r.dialogue_text, \"reference\": r.reference_summary_1})\n",
    "        rows.append({\"id\": r.id, \"topic\": r.topic, \"candidate\": r.dialogue_text, \"reference\": r.reference_summary_2})\n",
    "    df_long = pd.DataFrame(rows)\n",
    "    df_long.to_csv(TARGET_PATH, index=False)\n",
    "    print(\"✅ Rebuilt:\", TARGET_PATH)\n",
    "\n",
    "print(\"FINAL_PATH =\", TARGET_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee378835-f3ca-467d-a563-1fae3ccf323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:53:04.562270: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-30 10:53:28.102973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 10:53:41.378197: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (176, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>candidate</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manual_001</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>JAMES: Good morning, Professor Austin, how are...</td>\n",
       "      <td>James introduces Emma to Professor Austin and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual_001</td>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>JAMES: Good morning, Professor Austin, how are...</td>\n",
       "      <td>James greets Professor Austin, presents Emma, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             topic  \\\n",
       "0  manual_001  formal_greetings   \n",
       "1  manual_001  formal_greetings   \n",
       "\n",
       "                                           candidate  \\\n",
       "0  JAMES: Good morning, Professor Austin, how are...   \n",
       "1  JAMES: Good morning, Professor Austin, how are...   \n",
       "\n",
       "                                           reference  \n",
       "0  James introduces Emma to Professor Austin and ...  \n",
       "1  James greets Professor Austin, presents Emma, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bleurt import score as bleurt_score\n",
    "\n",
    "# ✅ use the correct absolute or relative path\n",
    "BLEURT_READY = \"/workspaces/bleurt/dialoguesum_bleurt_ready/dialoguesum_bleurt_ready.csv\"\n",
    "\n",
    "df = pd.read_csv(BLEURT_READY)\n",
    "print(\"Loaded:\", df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6065ec56-d5a0-4e1b-b297-bef64e4d31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scored: dialoguesum_bleurt_scored.csv\n",
      "Using annot : dialoguesum_lite_100_clean.csv\n",
      "\n",
      "Overall BLEURT stats:\n",
      "  n = 0\n",
      "  mean = NA\n",
      "  std  = NA\n",
      "\n",
      "Overall human score stats:\n",
      "  n = 100\n",
      "  mean = 2.2100\n",
      "  std  = 0.4333\n",
      "\n",
      "Per-topic BLEURT (first 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>n</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [topic, n, mean, std]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-topic human scores (first 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>n</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank_inquiry</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call_center</td>\n",
       "      <td>3</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classroom</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doctor_visit</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emergency</td>\n",
       "      <td>3</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flight_change</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>formal_greetings</td>\n",
       "      <td>4</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>formal_introductions</td>\n",
       "      <td>5</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hotel_booking</td>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>immigration_enquiry</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  topic  n      mean      std\n",
       "0          bank_inquiry  4  2.000000  0.00000\n",
       "1           call_center  3  2.333333  0.57735\n",
       "2             classroom  4  2.000000  0.00000\n",
       "3          doctor_visit  4  2.000000  0.00000\n",
       "4             emergency  3  2.000000  0.00000\n",
       "5         flight_change  4  2.000000  0.00000\n",
       "6      formal_greetings  4  2.750000  0.50000\n",
       "7  formal_introductions  5  2.000000  0.00000\n",
       "8         hotel_booking  4  2.500000  0.57735\n",
       "9   immigration_enquiry  4  2.000000  0.00000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def first_existing(paths):\n",
    "    for p in map(Path, paths):\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def coerce_numeric(s):\n",
    "    # convert to numeric safely and drop NaNs\n",
    "    return pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "\n",
    "def series_stats(s):\n",
    "    s = coerce_numeric(s)\n",
    "    n = int(s.size)\n",
    "    mean = float(s.mean()) if n else np.nan\n",
    "    std  = float(s.std(ddof=1)) if n > 1 else np.nan  # sample std\n",
    "    return {\"n\": n, \"mean\": mean, \"std\": std}\n",
    "\n",
    "# ---------- locate files (lightweight & robust) ----------\n",
    "scored = first_existing([\n",
    "    \"dialoguesum_bleurt_scored_safe.csv\",\n",
    "    \"dialoguesum_bleurt_scored.csv\",\n",
    "    \"/workspaces/bleurt/dialoguesum_bleurt_scored_safe.csv\",\n",
    "    \"/workspaces/bleurt/dialoguesum_bleurt_scored.csv\",\n",
    "    \"outputs/dialoguesum_bleurt_scored_safe.csv\",\n",
    "])\n",
    "\n",
    "annot  = first_existing([\n",
    "    \"dialoguesum_lite_100_clean.csv\",\n",
    "    \"/workspaces/bleurt/dialoguesum_lite_100_clean.csv\",\n",
    "    \"data/dialoguesum_lite_100_clean.csv\",\n",
    "])\n",
    "\n",
    "assert scored is not None, \"Couldn't find a scored CSV (looked for *_scored_safe.csv or *_scored.csv).\"\n",
    "assert annot  is not None, \"Couldn't find dialoguesum_lite_100_clean.csv.\"\n",
    "\n",
    "print(\"Using scored:\", scored)\n",
    "print(\"Using annot :\", annot)\n",
    "\n",
    "# ---------- load minimal columns ----------\n",
    "sc = pd.read_csv(scored)\n",
    "an = pd.read_csv(annot)\n",
    "\n",
    "# column detection (accept common variants)\n",
    "bleurt_col = next(c for c in [\"bleurt\",\"bleurt_score\",\"BLEURT\",\"score\",\"score_bleurt\"] if c in sc.columns)\n",
    "human_col  = next(c for c in [\"human_score\",\"avg_human\",\"rating\",\"mean_rating\"] if c in an.columns)\n",
    "\n",
    "# ---------- overall stats ----------\n",
    "bleurt_stats = series_stats(sc[bleurt_col])\n",
    "human_stats  = series_stats(an[human_col])\n",
    "\n",
    "print(\"\\nOverall BLEURT stats:\")\n",
    "print(f\"  n = {bleurt_stats['n']}\")\n",
    "print(f\"  mean = {bleurt_stats['mean']:.4f}\" if bleurt_stats['n'] else \"  mean = NA\")\n",
    "print(f\"  std  = {bleurt_stats['std']:.4f}\"  if bleurt_stats['n']>1 else \"  std  = NA\")\n",
    "\n",
    "print(\"\\nOverall human score stats:\")\n",
    "print(f\"  n = {human_stats['n']}\")\n",
    "print(f\"  mean = {human_stats['mean']:.4f}\" if human_stats['n'] else \"  mean = NA\")\n",
    "print(f\"  std  = {human_stats['std']:.4f}\"  if human_stats['n']>1 else \"  std  = NA\")\n",
    "\n",
    "# ---------- optional: per-topic stats if 'topic' exists in each file ----------\n",
    "if \"topic\" in sc.columns:\n",
    "    bt = (sc[[\"topic\", bleurt_col]]\n",
    "          .assign(val=lambda d: pd.to_numeric(d[bleurt_col], errors=\"coerce\"))\n",
    "          .dropna(subset=[\"val\"])\n",
    "          .groupby(\"topic\")[\"val\"]\n",
    "          .agg(n=\"size\", mean=\"mean\", std=lambda x: x.std(ddof=1))\n",
    "          .reset_index())\n",
    "    print(\"\\nPer-topic BLEURT (first 10 rows):\")\n",
    "    display(bt.head(10))\n",
    "\n",
    "if \"topic\" in an.columns:\n",
    "    ht = (an[[\"topic\", human_col]]\n",
    "          .assign(val=lambda d: pd.to_numeric(d[human_col], errors=\"coerce\"))\n",
    "          .dropna(subset=[\"val\"])\n",
    "          .groupby(\"topic\")[\"val\"]\n",
    "          .agg(n=\"size\", mean=\"mean\", std=lambda x: x.std(ddof=1))\n",
    "          .reset_index())\n",
    "    print(\"\\nPer-topic human scores (first 10 rows):\")\n",
    "    display(ht.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177155da-1ed8-439a-8f6f-a1e93f12b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialoguesum_lite_100_clean Results:\n",
      "Mean Human Score: 2.2100\n",
      "Standard Deviation: 0.4333\n",
      "N: 100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === 1) Path to your original file ===\n",
    "data_path = Path(\"dialoguesum_lite_100_clean.csv\")   # ← your actual file name\n",
    "\n",
    "# === 2) Load safely ===\n",
    "assert data_path.exists(), f\"File not found: {data_path}\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# === 3) Detect the human-score column automatically ===\n",
    "score_col = next((c for c in [\"human_score\",\"avg_human\",\"rating\",\"mean_rating\"] if c in df.columns), None)\n",
    "assert score_col is not None, f\"No human score column found in {data_path.name}\"\n",
    "\n",
    "# === 4) Compute descriptive statistics ===\n",
    "scores = pd.to_numeric(df[score_col], errors=\"coerce\").dropna()\n",
    "n = len(scores)\n",
    "mean = scores.mean()\n",
    "std = scores.std(ddof=1) if n > 1 else np.nan\n",
    "\n",
    "print(\"Dialoguesum_lite_100_clean Results:\")\n",
    "print(f\"Mean Human Score: {mean:.4f}\" if n else \"Mean Human Score: NA\")\n",
    "print(f\"Standard Deviation: {std:.4f}\" if n > 1 else \"Standard Deviation: NA\")\n",
    "print(f\"N: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9b6396-fe24-4eab-85da-17ce74116cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No BLEURT scores found → using ROUGE-L recall proxy.\n",
      "\n",
      "Dialoguesum correlations (ROUGE-L recall (proxy) vs human_score):\n",
      "  N            : 100\n",
      "  Pearson r    : -0.0290 (p=7.74e-01)\n",
      "  Spearman ρ   : -0.0344 (p=7.34e-01)\n",
      "  Kendall τ-b  : -0.0323 (p=7.28e-01)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# ---------- 0) Paths ----------\n",
    "DATA = Path(\"dialoguesum_lite_100_clean.csv\")\n",
    "SC_SCORED = [Path(\"dialoguesum_bleurt_scored_safe.csv\"),\n",
    "             Path(\"dialoguesum_bleurt_scored.csv\")]\n",
    "assert DATA.exists(), f\"Missing {DATA}\"\n",
    "\n",
    "# ---------- 1) Load your dataset ----------\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "# Column detection\n",
    "id_col = next((c for c in [\"id\",\"example_id\",\"idx\",\"sample_id\",\"_row_id\"] if c in df.columns), None)\n",
    "cand_col = next((c for c in [\"candidate_summary\",\"candidate\",\"system_summary\"] if c in df.columns), None)\n",
    "ref_cols = [c for c in [\"reference_summary\",\"reference_summary_1\",\"reference_summary_2\"] if c in df.columns]\n",
    "human_col = next((c for c in [\"human_score\",\"avg_human\",\"rating\",\"mean_rating\"] if c in df.columns), None)\n",
    "assert cand_col and ref_cols and human_col, \"Need candidate/ref(s)/human_score columns.\"\n",
    "\n",
    "# ---------- 2) Try to get BLEURT without re-scoring ----------\n",
    "bleurt_series = None\n",
    "\n",
    "# 2a) If BLEURT already inside the same CSV\n",
    "for bc in [\"bleurt\",\"bleurt_score\",\"BLEURT\",\"score_bleurt\"]:\n",
    "    if bc in df.columns:\n",
    "        bleurt_series = pd.to_numeric(df[bc], errors=\"coerce\")\n",
    "        break\n",
    "\n",
    "# 2b) Else look for sidecar scored CSV and align (cheap, no TF)\n",
    "if bleurt_series is None:\n",
    "    for p in SC_SCORED:\n",
    "        if not p.exists(): \n",
    "            continue\n",
    "        sc = pd.read_csv(p)\n",
    "        # choose columns we need if present\n",
    "        sc_id = next((c for c in [\"id\",\"example_id\",\"idx\",\"sample_id\"] if c in sc.columns), None)\n",
    "        sc_bleurt = next((c for c in [\"bleurt\",\"bleurt_score\",\"BLEURT\",\"score_bleurt\"] if c in sc.columns), None)\n",
    "        # fast path: join by id if both have ids\n",
    "        if sc_id and id_col and sc_bleurt:\n",
    "            m = (df[[id_col]].merge(sc[[sc_id, sc_bleurt]], left_on=id_col, right_on=sc_id, how=\"left\"))\n",
    "            bleurt_series = pd.to_numeric(m[sc_bleurt], errors=\"coerce\")\n",
    "            if bleurt_series.notna().any():\n",
    "                break\n",
    "        # fallback: join by normalized text signature (candidate+reference)\n",
    "        if sc_bleurt and {\"candidate\",\"reference\"}.issubset(sc.columns):\n",
    "            def sig(c,r): \n",
    "                return (c.astype(str).str.lower().str.replace(r\"\\s+\",\" \",regex=True).str.strip()\n",
    "                        + \"||\" + r.astype(str).str.lower().str.replace(r\"\\s+\",\" \",regex=True).str.strip())\n",
    "            # build signatures on a *single* reference (first available) for a cheap join\n",
    "            sig_df = sig(df[cand_col], df[ref_cols[0]])\n",
    "            sig_sc = sig(sc[\"candidate\"], sc[\"reference\"])\n",
    "            tmp = pd.DataFrame({\"sig\": sig_df})\n",
    "            tmp2 = pd.DataFrame({\"sig\": sig_sc, \"bleurt\": sc[sc_bleurt]})\n",
    "            m = tmp.merge(tmp2, on=\"sig\", how=\"left\")\n",
    "            bleurt_series = pd.to_numeric(m[\"bleurt\"], errors=\"coerce\")\n",
    "            if bleurt_series.notna().any():\n",
    "                break\n",
    "\n",
    "# ---------- 3) If BLEURT still missing, compute a tiny ROUGE-L recall proxy ----------\n",
    "def rougeL_recall(candidate: str, reference: str) -> float:\n",
    "    # whitespace-token LCS (O(n*m)); fine for ~100 examples\n",
    "    a = candidate.split(); b = reference.split()\n",
    "    if not a or not b: \n",
    "        return 0.0\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [0]*(m+1)\n",
    "    for i in range(1, n+1):\n",
    "        prev = 0\n",
    "        for j in range(1, m+1):\n",
    "            t = dp[j]\n",
    "            if a[i-1] == b[j-1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = dp[j] if dp[j] > dp[j-1] else dp[j-1]\n",
    "            prev = t\n",
    "    lcs = dp[m]\n",
    "    return lcs / m\n",
    "\n",
    "if bleurt_series is None or bleurt_series.isna().all():\n",
    "    print(\"No BLEURT scores found → using ROUGE-L recall proxy.\")\n",
    "    # Use first available reference for a consistent target\n",
    "    rcol = ref_cols[0]\n",
    "    # compute quickly (vectorized-ish loop; still light for ~100)\n",
    "    x_scores = []\n",
    "    for cand, ref in zip(df[cand_col].astype(str), df[rcol].astype(str)):\n",
    "        x_scores.append(rougeL_recall(cand.strip(), ref.strip()))\n",
    "    metric = pd.Series(x_scores, dtype=float, name=\"metric\")\n",
    "    target_name = \"ROUGE-L recall (proxy)\"\n",
    "else:\n",
    "    metric = pd.to_numeric(bleurt_series, errors=\"coerce\")\n",
    "    target_name = \"BLEURT\"\n",
    "\n",
    "# ---------- 4) Prepare human scores and drop NaNs ----------\n",
    "human = pd.to_numeric(df[human_col], errors=\"coerce\")\n",
    "mask = metric.notna() & human.notna()\n",
    "x = metric[mask].to_numpy()\n",
    "y = human[mask].to_numpy()\n",
    "assert len(x) > 2, \"Not enough valid pairs to correlate.\"\n",
    "\n",
    "# ---------- 5) Correlations (fast; no bootstrap to stay ultra-light) ----------\n",
    "pr, pp = stats.pearsonr(x, y)\n",
    "sr, sp = stats.spearmanr(x, y)\n",
    "kt, kp = stats.kendalltau(x, y, variant=\"b\")\n",
    "\n",
    "print(f\"\\nDialoguesum correlations ({target_name} vs human_score):\")\n",
    "print(f\"  N            : {len(x)}\")\n",
    "print(f\"  Pearson r    : {pr:.4f} (p={pp:.2e})\")\n",
    "print(f\"  Spearman ρ   : {sr:.4f} (p={sp:.2e})\")\n",
    "print(f\"  Kendall τ-b  : {kt:.4f} (p={kp:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d453-1920-4831-ac4a-6730bc852297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
