{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f520d7-f014-46f1-a21e-554b64d13c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 06:21:27.083757: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-04 06:21:52.004665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 06:22:04.916635: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json, math, zipfile, io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from pathlib import Path                      # <-- fixes NameError: Path\n",
    "\n",
    "from bleurt import score as bleurt_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac32d1c-d04a-4924-bb63-9896b542075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspaces/bleurt\n",
      "Output directory : /workspaces/bleurt/bleurt_eval_outputs\n",
      "Checkpoint exists: True\n"
     ]
    }
   ],
   "source": [
    "# ========= CHECKPOINT =========\n",
    "# You already have these folders next to your notebooks (per your screenshot)\n",
    "BLEURT_CHECKPOINT = \"./BLEURT-20\"            # or \"./BLEURT-20-D12\"\n",
    "\n",
    "# If you ever want the notebook to auto-download, keep it writable (no /mnt/data)\n",
    "DOWNLOAD_BLEURT   = False\n",
    "BLEURT_ZIP_URL    = \"https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\"\n",
    "BLEURT_ZIP_DST    = str(Path.cwd() / \"BLEURT-20.zip\")   # <-- writable\n",
    "\n",
    "# ========= DATA: PATH A (recommended) =========\n",
    "# Put the WebNLG CSV in the same folder as your notebook (or adjust path)\n",
    "WEBNLG_CSV = \"./all_data_final_averaged.csv\"\n",
    "\n",
    "# For WMT you can leave these empty for now, or point them to files you have\n",
    "WMT_INPUTS = [\n",
    "    # e.g., \"./wmt19_ende_segments.tsv\",\n",
    "]\n",
    "WMT_GLOB = None                                # or e.g., \"./wmt19/**/*segments*.tsv\"\n",
    "\n",
    "# If you later use mt-metrics-eval locally, keep paths writable (no /mnt/data)\n",
    "MTMETRICS_ROOT     = str(Path.cwd() / \"mt-metrics-eval\")\n",
    "USE_MT_METRICS_EVAL = False\n",
    "MTMETRICS_SET      = \"wmt19\"\n",
    "\n",
    "# ========= COLUMN MAPPINGS =========\n",
    "WEBNLG_COLMAP = {\"system\":\"system\",\"reference\":\"reference\",\n",
    "                 \"candidate\":\"candidate\",\"human\":\"human_score\"}\n",
    "WMT_COLMAP    = {\"system\":\"system\",\"reference\":\"reference\",\n",
    "                 \"candidate\":\"candidate\",\"human\":\"human_score\",\n",
    "                 \"langpair\": None}\n",
    "\n",
    "WEBNLG_FILTER_QUERY = None\n",
    "WMT_FILTER_QUERY    = None\n",
    "\n",
    "# ========= OUTPUTS (writable) =========\n",
    "OUT_DIR = Path.cwd() / \"bleurt_eval_outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)      # <-- no permission error\n",
    "\n",
    "# ========= PERFORMANCE =========\n",
    "BLEURT_BATCH_SIZE = 64\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Output directory :\", OUT_DIR)\n",
    "print(\"Checkpoint exists:\", Path(BLEURT_CHECKPOINT).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed423a9-2970-410b-bad8-779029ecdf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-05 06:06:50--  https://gitlab.com/webnlg/webnlg-human-evaluation/-/raw/master/all_data_final_averaged.csv\n",
      "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
      "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 806795 (788K) [text/plain]\n",
      "Saving to: ‘all_data_final_averaged.csv’\n",
      "\n",
      "all_data_final_aver 100%[===================>] 787.89K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2025-10-05 06:06:50 (70.1 MB/s) - ‘all_data_final_averaged.csv’ saved [806795/806795]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O all_data_final_averaged.csv \\\n",
    "  https://gitlab.com/webnlg/webnlg-human-evaluation/-/raw/master/all_data_final_averaged.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d5908a-4b49-4aa1-837b-e16011d49f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example snippet\u001b[39;00m\n\u001b[32m      2\u001b[39m refs_long = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m id_ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_h\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m].unique()[:\u001b[32m50\u001b[39m]:\n\u001b[32m      4\u001b[39m     refs = fetch_references_for_id(id_)   \u001b[38;5;66;03m# from WebNLG official repo or your local copy\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m refs:\n",
      "\u001b[31mNameError\u001b[39m: name 'df_h' is not defined"
     ]
    }
   ],
   "source": [
    "# Example snippet\n",
    "refs_long = []\n",
    "for id_ in df_h[\"id\"].unique()[:50]:\n",
    "    refs = fetch_references_for_id(id_)   # from WebNLG official repo or your local copy\n",
    "    for r in refs:\n",
    "        refs_long.append({\"id\": id_, \"reference\": r})\n",
    "pd.DataFrame(refs_long).to_csv(\"refs_long.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97082f8-8128-48c4-8726-2d7cdffb4638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refs_long.csv created in current directory\n",
      "   id                                 reference\n",
      "0   1     The Eiffel Tower is located in Paris.\n",
      "1   1        Paris is home to the Eiffel Tower.\n",
      "2   2  The Nile is the longest river in Africa.\n",
      "3   2        The Nile river runs through Egypt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example references\n",
    "data = {\n",
    "    \"id\": [1, 1, 2, 2],\n",
    "    \"reference\": [\n",
    "        \"The Eiffel Tower is located in Paris.\",\n",
    "        \"Paris is home to the Eiffel Tower.\",\n",
    "        \"The Nile is the longest river in Africa.\",\n",
    "        \"The Nile river runs through Egypt.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "refs_long = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV in the current notebook folder\n",
    "refs_long.to_csv(\"refs_long.csv\", index=False)\n",
    "\n",
    "print(\"refs_long.csv created in current directory\")\n",
    "print(refs_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516e82f4-7c2c-4a42-8b21-ea270f4681d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, tempfile, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "# ========= CONFIG (BLEURT-20, small CLI batch, tiny chunks for stability) =========\n",
    "CKPT       = \"./BLEURT-20\"   # full model used in the article\n",
    "CLI_BATCH  = 8               # CLI --batch_size\n",
    "CHUNK_SIZE = 40              # score 40 pairs per subprocess call (keeps RAM low)\n",
    "SEED       = 42\n",
    "\n",
    "# Paths (edit if needed)\n",
    "HUMAN_EVAL_CSV = \"./all_data_final_averaged.csv\"  # you already downloaded this\n",
    "REFS_LONG_CSV  = \"./refs_long.csv\"                # <-- provide (id, reference) long-format CSV\n",
    "\n",
    "# ---------- Load human eval (WebNLG 2017) ----------\n",
    "df_h = pd.read_csv(HUMAN_EVAL_CSV)\n",
    "# Map to required columns: system, candidate, human_score (semantics)\n",
    "df_h = df_h.rename(columns={\n",
    "    \"team\": \"system\",\n",
    "    \"text\": \"candidate\",\n",
    "    \"semantics\": \"human_score\"\n",
    "})\n",
    "need_cols = {\"id\",\"system\",\"candidate\",\"human_score\"}\n",
    "assert need_cols.issubset(df_h.columns), f\"Missing columns in {HUMAN_EVAL_CSV}. Need {need_cols}\"\n",
    "df_h[\"human_score\"] = pd.to_numeric(df_h[\"human_score\"], errors=\"coerce\")\n",
    "df_h = df_h.dropna(subset=[\"system\",\"candidate\",\"human_score\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- Load textual references (long: id, reference) ----------\n",
    "if not Path(REFS_LONG_CSV).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"{REFS_LONG_CSV} not found.\\n\"\n",
    "        \"Create refs_long.csv with columns: id,reference (one row per reference).\"\n",
    "    )\n",
    "refs_long = pd.read_csv(REFS_LONG_CSV)\n",
    "assert {\"id\",\"reference\"}.issubset(refs_long.columns), \"refs_long.csv must have columns: id, reference\"\n",
    "refs_long[\"reference\"] = refs_long[\"reference\"].astype(str).str.strip()\n",
    "refs_long = refs_long[refs_long[\"reference\"]!=\"\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabd8569-4f1e-4965-a600-fbb20cd02da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows with textual refs: 4\n",
      "Systems: 2 | Unique candidates: 2\n"
     ]
    }
   ],
   "source": [
    "# Join human-eval rows to textual references (many refs per id)\n",
    "df = df_h.merge(refs_long, on=\"id\", how=\"inner\")\n",
    "df = df.dropna(subset=[\"system\",\"candidate\",\"reference\",\"human_score\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Joined rows with textual refs: {len(df)}\")\n",
    "print(\"Systems:\", df['system'].nunique(), \"| Unique candidates:\", df[['id','system','candidate']].drop_duplicates().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cacfd30-07f5-4629-9c2f-30943b398ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Make checkpoint path absolute\n",
    "CKPT = str(Path(\"./BLEURT-20\").resolve())  # or \"./BLEURT-20-D12\"\n",
    "\n",
    "def bleurt_subprocess_scores(refs, cands, ckpt=CKPT, batch=8):\n",
    "    \"\"\"Scores in a separate Python process using BLEURT's Python API.\n",
    "       Keeps TF memory out of your notebook; always writes scores file.\"\"\"\n",
    "    assert len(refs) == len(cands), \"refs and cands must have same length\"\n",
    "    if not refs:\n",
    "        return np.zeros(0, dtype=\"float32\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpd:\n",
    "        tmpd = Path(tmpd)\n",
    "        refs_p  = (tmpd / \"refs.txt\").resolve()\n",
    "        cands_p = (tmpd / \"cands.txt\").resolve()\n",
    "        out_p   = (tmpd / \"scores.txt\").resolve()\n",
    "        script  = (tmpd / \"run_bleurt.py\").resolve()\n",
    "\n",
    "        # write inputs\n",
    "        refs_p.write_text(\"\\n\".join(refs),  encoding=\"utf-8\")\n",
    "        cands_p.write_text(\"\\n\".join(cands), encoding=\"utf-8\")\n",
    "\n",
    "        # write a tiny Python script that uses BLEURT API\n",
    "        script.write_text(f\"\"\"\n",
    "import os, sys\n",
    "from bleurt import score as bleurt_score\n",
    "ckpt, refs_f, cands_f, out_f, bs = sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], int(sys.argv[5])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "with open(refs_f, \"r\", encoding=\"utf-8\") as f:\n",
    "    refs = [l.rstrip(\"\\\\n\") for l in f]\n",
    "with open(cands_f, \"r\", encoding=\"utf-8\") as f:\n",
    "    cands = [l.rstrip(\"\\\\n\") for l in f]\n",
    "\n",
    "scorer = bleurt_score.BleurtScorer(ckpt)\n",
    "scores = []\n",
    "for i in range(0, len(refs), bs):\n",
    "    scores.extend(scorer.score(references=refs[i:i+bs], candidates=cands[i:i+bs]))\n",
    "\n",
    "with open(out_f, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in scores:\n",
    "        f.write(f\"{{s}}\\\\n\")\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "        # run it\n",
    "        env = os.environ.copy()\n",
    "        env[\"CUDA_VISIBLE_DEVICES\"]   = \"-1\"\n",
    "        env[\"TF_CPP_MIN_LOG_LEVEL\"]   = \"2\"\n",
    "        env[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "        env[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "        cmd = [sys.executable, str(script), str(Path(ckpt).resolve()),\n",
    "               str(refs_p), str(cands_p), str(out_p), str(batch)]\n",
    "        cp = subprocess.run(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        if cp.returncode != 0:\n",
    "            raise RuntimeError(f\"BLEURT subprocess failed (code {cp.returncode}).\\nSTDOUT:\\n{cp.stdout}\\n\\nSTDERR:\\n{cp.stderr}\")\n",
    "\n",
    "        if not out_p.exists():\n",
    "            raise RuntimeError(f\"BLEURT subprocess produced no scores file.\\nCMD: {' '.join(cmd)}\\nSTDOUT:\\n{cp.stdout}\\n\\nSTDERR:\\n{cp.stderr}\")\n",
    "\n",
    "        lines = [ln for ln in out_p.read_text(encoding=\"utf-8\").splitlines() if ln.strip() != \"\"]\n",
    "        if len(lines) != len(refs):\n",
    "            raise RuntimeError(f\"Scores count mismatch: expected {len(refs)} got {len(lines)}.\\nFirst lines: {lines[:5]}\\nSTDERR:\\n{cp.stderr}\")\n",
    "\n",
    "        return np.array([float(x) for x in lines], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d4c7b4-faab-4654-8c4d-bda5fe6d579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe: [1.0015844]\n"
     ]
    }
   ],
   "source": [
    "print(\"Probe:\", bleurt_subprocess_scores(\n",
    "    [\"The Eiffel Tower is in Paris.\"],\n",
    "    [\"The Eiffel Tower is in Paris.\"],\n",
    "    batch=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1f48252-de67-4f65-98fa-1f1446abcc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows (with text refs): 4\n",
      "Systems: 2 | Unique candidates: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Human eval (you already have this file)\n",
    "df_h = pd.read_csv(\"./all_data_final_averaged.csv\").rename(columns={\n",
    "    \"team\": \"system\",\n",
    "    \"text\": \"candidate\",\n",
    "    \"semantics\": \"human_score\",\n",
    "})\n",
    "df_h[\"human_score\"] = pd.to_numeric(df_h[\"human_score\"], errors=\"coerce\")\n",
    "df_h = df_h.dropna(subset=[\"id\",\"system\",\"candidate\",\"human_score\"]).reset_index(drop=True)\n",
    "\n",
    "# Your textual references created earlier: refs_long.csv (id, reference)\n",
    "refs_long = pd.read_csv(\"refs_long.csv\")   # must have columns: id, reference\n",
    "refs_long[\"reference\"] = refs_long[\"reference\"].astype(str).str.strip()\n",
    "refs_long = refs_long[refs_long[\"reference\"] != \"\"].copy()\n",
    "\n",
    "# Join: each candidate paired with ALL its refs\n",
    "df = (df_h.merge(refs_long, on=\"id\", how=\"inner\")\n",
    "          .dropna(subset=[\"reference\"])\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "print(f\"Joined rows (with text refs): {len(df)}\")\n",
    "print(\"Systems:\", df[\"system\"].nunique(),\n",
    "      \"| Unique candidates:\", df[[\"id\",\"system\",\"candidate\"]].drop_duplicates().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a5b473-9069-48f0-8bae-cf14c6539ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 2 candidates with max-over-refs…\n",
      "Candidates after max-over-refs: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system</th>\n",
       "      <th>candidate</th>\n",
       "      <th>human_score</th>\n",
       "      <th>bleurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>adapt</td>\n",
       "      <td>the 29075 club is the dictcoverer, carl a. wir...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.143762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>baseline</td>\n",
       "      <td>the administrative government is governed by t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    system                                          candidate  \\\n",
       "0   1     adapt  the 29075 club is the dictcoverer, carl a. wir...   \n",
       "1   2  baseline  the administrative government is governed by t...   \n",
       "\n",
       "   human_score    bleurt  \n",
       "0     1.333333  0.143762  \n",
       "1     1.000000  0.231049  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "CHUNK_SIZE = 40    # safe on CPU; adjust up/down if needed\n",
    "BATCH      = 8     # per subprocess\n",
    "\n",
    "key_cols = [\"id\",\"system\",\"candidate\",\"human_score\"]\n",
    "agg_rows = []\n",
    "\n",
    "groups = df.groupby(key_cols, sort=False)\n",
    "print(f\"Scoring {len(groups)} candidates with max-over-refs…\")\n",
    "\n",
    "for (gid, sysname, cand, human), grp in groups:\n",
    "    refs_list = grp[\"reference\"].tolist()\n",
    "    scores_all = []\n",
    "    for i in range(0, len(refs_list), CHUNK_SIZE):\n",
    "        rchunk = refs_list[i:i+CHUNK_SIZE]\n",
    "        cchunk = [cand] * len(rchunk)\n",
    "        scores_all.extend(bleurt_subprocess_scores(rchunk, cchunk, batch=BATCH))\n",
    "    bleurt_max = float(np.max(np.array(scores_all, dtype=\"float32\")))\n",
    "    agg_rows.append((gid, sysname, cand, human, bleurt_max))\n",
    "\n",
    "df_agg = pd.DataFrame(agg_rows, columns=[\"id\",\"system\",\"candidate\",\"human_score\",\"bleurt\"])\n",
    "print(\"Candidates after max-over-refs:\", len(df_agg))\n",
    "df_agg.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eef1b71-5034-4fe6-a323-d2ef96147355",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_agg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pearsonr, kendalltau\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Segment-level\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m r, r_p  = pearsonr(\u001b[43mdf_agg\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mhuman_score\u001b[39m\u001b[33m\"\u001b[39m], df_agg[\u001b[33m\"\u001b[39m\u001b[33mbleurt\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m t, t_p  = kendalltau(df_agg[\u001b[33m\"\u001b[39m\u001b[33mhuman_score\u001b[39m\u001b[33m\"\u001b[39m], df_agg[\u001b[33m\"\u001b[39m\u001b[33mbleurt\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# System-level (headline in the paper)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_agg' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "# Segment-level\n",
    "r, r_p  = pearsonr(df_agg[\"human_score\"], df_agg[\"bleurt\"])\n",
    "t, t_p  = kendalltau(df_agg[\"human_score\"], df_agg[\"bleurt\"])\n",
    "\n",
    "# System-level (headline in the paper)\n",
    "sys_tbl = (df_agg.groupby(\"system\", as_index=False)\n",
    "                    .agg(human_mean=(\"human_score\",\"mean\"),\n",
    "                         bleurt_mean=(\"bleurt\",\"mean\"),\n",
    "                         n=(\"bleurt\",\"size\")))\n",
    "\n",
    "sr, sr_p = pearsonr(sys_tbl[\"human_mean\"], sys_tbl[\"bleurt_mean\"]) if len(sys_tbl)>1 else (np.nan, np.nan)\n",
    "st, st_p = kendalltau(sys_tbl[\"human_mean\"], sys_tbl[\"bleurt_mean\"]) if len(sys_tbl)>1 else (np.nan, np.nan)\n",
    "\n",
    "print(\"\\n=== WEBNLG (text refs, BLEURT-20, max-over-refs) ===\")\n",
    "print(f\"Candidates: {len(df_agg)} | Systems: {len(sys_tbl)}\")\n",
    "print(f\"Segment:  Pearson r={r:.100f} (p={r_p:.50e}) | Kendall τ={t:.100f} (p={t_p:.50e})\")\n",
    "print(f\"System :  Pearson r={sr:.100f} (p={sr_p:.50e}) | Kendall τ={st:.100f} (p={st_p:.50e})\")\n",
    "\n",
    "# Save\n",
    "OUT = Path(\"bleurt_eval_outputs\"); OUT.mkdir(exist_ok=True)\n",
    "df_agg.to_csv(OUT/\"webnlg_bleurt20_textrefs_scores.csv\", index=False)\n",
    "sys_tbl.to_csv(OUT/\"webnlg_bleurt20_textrefs_system_means.csv\", index=False)\n",
    "print(\"Saved to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cae2676-7aa8-406e-80eb-d479ae9777c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows: 4 | systems: 2 | candidates: 2\n"
     ]
    }
   ],
   "source": [
    "# pick up to K ids per system that are present in refs_long\n",
    "K = 20  # increase as you get more refs\n",
    "ids_with_refs = set(refs_long[\"id\"].unique())\n",
    "df_h_sub = df_h[df_h[\"id\"].isin(ids_with_refs)].copy()\n",
    "\n",
    "chosen = []\n",
    "for sys, grp in df_h_sub.groupby(\"system\", sort=False):\n",
    "    chosen.extend(grp.head(K).index.tolist())\n",
    "df_h_sub = df_h_sub.loc[chosen].reset_index(drop=True)\n",
    "\n",
    "# join to refs and proceed exactly as before\n",
    "df = (df_h_sub.merge(refs_long, on=\"id\", how=\"inner\")\n",
    "               .dropna(subset=[\"reference\"])\n",
    "               .reset_index(drop=True))\n",
    "print(f\"Joined rows: {len(df)} | systems: {df['system'].nunique()} | candidates: {df[['id','system','candidate']].drop_duplicates().shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf80a03-fe7c-409d-9cbb-0dc9ad788544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY: use MR as a reference (NOT article-comparable, but good to check scale)\n",
    "df_tmp = df_h.rename(columns={\"mr\":\"reference\"}) \\\n",
    "            .dropna(subset=[\"reference\"]) \\\n",
    "            .reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9c9e9-e29d-47fa-ad7a-b441417b8a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
